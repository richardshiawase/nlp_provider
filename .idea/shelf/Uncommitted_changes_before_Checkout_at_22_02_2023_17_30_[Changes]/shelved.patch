Index: nlp_provider/views.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import json\r\nimport os\r\nimport pathlib\r\nimport pickle\r\nimport re\r\nimport shutil\r\nfrom collections import defaultdict\r\nfrom functools import reduce\r\nfrom multiprocessing import Process, Pool\r\n\r\nimport numpy as np\r\nimport requests\r\nfrom django.core import serializers\r\nfrom django.core.files.storage import FileSystemStorage\r\nfrom django.core.paginator import Paginator, PageNotAnInteger, EmptyPage\r\nfrom django.http import HttpResponse\r\nfrom django.shortcuts import render\r\nimport warnings\r\nwarnings.simplefilter(action='ignore', category=FutureWarning)\r\nimport pandas as pd\r\nfrom requests import Response\r\nfrom django.http import JsonResponse\r\nfrom sklearn import metrics\r\nfrom sklearn.feature_extraction.text import TfidfVectorizer\r\nfrom sklearn.metrics import precision_score, f1_score, accuracy_score\r\nimport django\r\ndjango.setup()\r\nfrom model import models\r\nfrom model.views import create_model\r\nfrom .utils import ItemPembanding, Prediction, MasterData, PredictionId, Pembersih\r\nfrom model.models import Provider_Model, Perbandingan, Provider_Perbandingan\r\nfrom tqdm import tqdm\r\nfrom django.core.cache import cache\r\n# from .forms import UploadFileForm\r\n# Create your views here.\r\ndf_dataset = cache.get('dataset')\r\nif df_dataset is None:\r\n    df_dataset = pd.read_excel(\"dataset_excel_copy.xlsx\")\r\n    cache.set('dataset', df_dataset)\r\n\r\nnew_course_title = df_dataset['course_title'].str.lower().str.split(\"#\", n=1, expand=True)\r\ndf_dataset[\"course_titles\"] = new_course_title[0]\r\np = Pembersih((df_dataset.drop_duplicates(['course_title'], keep='first')))\r\ndf_non_duplicate = p._return_df()\r\nfilename = 'tfidf_vec.pickle'\r\ntfidf_vec1 = pickle.load(open(filename, 'rb'))\r\nfilename = 'finalized_model.sav'\r\nloaded_model1 = pickle.load(open(filename, 'rb'))\r\n\r\n\r\ndef index(request):\r\n\r\n    list_pembanding = []\r\n\r\n    pembanding_all = models.Perbandingan.objects.all()\r\n    for pembanding in pembanding_all:\r\n        pembanding.file_location = pembanding.file_location.split(\"media\")[1]\r\n        list_pembanding.append(pembanding)\r\n\r\n    context = {\"list_pembanding\":list_pembanding}\r\n    return render(request, 'home.html', context)\r\n\r\n\r\ndef kompilasi(request):\r\n\r\n\r\n    pembanding = models.Perbandingan.objects.all()\r\n    list_pembandinge = pembanding\r\n    list_pembanding = []\r\n    for pembanding in list_pembandinge:\r\n        pembanding.file_location = pembanding.file_location.split(\"media\")[1]\r\n        list_pembanding.append(pembanding)\r\n\r\n    context = {\"list_pembanding\":list_pembanding}\r\n\r\n    return render(request, 'kompilasi.html')\r\n\r\n\r\ndef kompilasi_data(request):\r\n    pembanding_all = models.Perbandingan.objects.all()\r\n    provider_list = []\r\n    for pembanding in pembanding_all:\r\n        pembanding.file_location = pembanding.file_location.split(\"media\")[1]\r\n        dfs = pd.read_excel(\"media/\"+pembanding.file_location_result)\r\n        for index, row in dfs.iterrows():\r\n            alamat = row['Alamat']\r\n            alamat_prediksi = row['Alamat Prediction']\r\n            ri = row['ri']\r\n            rj = row['rj']\r\n            item_obj = ItemPembanding(row['Provider Name'], row['Alamat'], row[\"Prediction\"], row[\"Score\"], 0,ri,rj)\r\n            item_obj.set_nama_asuransi(pembanding.nama_asuransi)\r\n            item_obj.set_selected(str(row['Compared']))\r\n            item_obj.set_alamat_prediction(alamat_prediksi)\r\n\r\n            provider_list.append(item_obj.__dict__)\r\n\r\n    return JsonResponse(provider_list, safe=False)\r\n\r\n\r\n\r\n\r\n\r\ndef newe(request):\r\n    data = list(models.Perbandingan.objects.values())\r\n    if request.method == \"GET\":\r\n        return JsonResponse(data, safe=False)\r\n\r\n    return JsonResponse({'message':'error'})\r\n\r\n\r\n\r\n\r\ndef perbandingan_rev(request):\r\n    global provider_liste\r\n    global file_location\r\n    provider_liste = []\r\n    response = requests.get('https://asateknologi.id/api/insuranceall')\r\n    response = response.json()\r\n    dfs = None\r\n    prediction_dict = {}\r\n    prediction_dict = defaultdict(lambda:0,prediction_dict)\r\n    if request.method == \"POST\":\r\n        file_location = \"media\"+request.POST[\"file_location\"]\r\n\r\n\r\n    try:\r\n        dfs = pd.read_excel(file_location)\r\n    except:\r\n        print(\"dataframe not founds\")\r\n    provider_list = []\r\n    if dfs is not None:\r\n        for index, row in dfs.iterrows():\r\n            provider_name = row['Provider Name']\r\n            y_preds = row[\"Prediction\"]\r\n            alamat = row['Alamat']\r\n            alamat_prediksi = row['Alamat Prediction']\r\n            nil = row[\"Score\"]\r\n            compared = row[\"Compared\"]\r\n            # ri = row['RI']\r\n            # rj = row['RJ']\r\n            # city = row[\"City\"]\r\n            # print(city)\r\n            prediction_dict[y_preds] += 1\r\n\r\n            provider_object = ItemPembanding(provider_name, alamat, y_preds, nil, 0,0,0)\r\n            provider_object.set_selected(compared)\r\n            provider_object.set_alamat_prediction(alamat_prediksi)\r\n            provider_list.append(provider_object.__dict__)\r\n\r\n\r\n    # MAP THE COUNT !\r\n    for provider_dict in provider_list:\r\n        for key, values in prediction_dict.items():\r\n            if(key == provider_dict[\"label_name\"]):\r\n                provider_dict['count_label_name'] = values\r\n\r\n\r\n    return JsonResponse(provider_list, safe=False)\r\n\r\n\r\n\r\ndef perbandingan(request):\r\n    global provider_liste\r\n    global file_location\r\n    # file_location = \"-\"\r\n    provider_liste = []\r\n    response = requests.get('https://asateknologi.id/api/insuranceall')\r\n    response = response.json()\r\n    dfs = None\r\n\r\n    if request.method == \"POST\":\r\n        file_location = \"media\"+request.POST[\"file_location\"]\r\n        print(file_location)\r\n        loop_delete(file_location)\r\n\r\n\r\n    # elif request.method == \"GET\":\r\n        # file_location=\"media/demo.xlsx\"\r\n    # else:\r\n    #     file_location = \"-\"\r\n    try:\r\n        dfs = pd.read_excel(file_location)\r\n\r\n    except Exception as e:\r\n        print(\"dataframe not founde \"+ str(e))\r\n    provider_list = []\r\n    if dfs is not None:\r\n        for index, row in dfs.iterrows():\r\n            provider_name = row['Provider Name']\r\n            y_preds = row[\"Prediction\"]\r\n            alamat = row[\"Alamat\"]\r\n            alamat_prediction = row[\"Alamat Prediction\"]\r\n            nil = row[\"Score\"]\r\n            compared = row[\"Compared\"]\r\n            # ri= row[\"RI\"]\r\n            # rj = row[\"RJ\"]\r\n            provider_object = ItemPembanding(provider_name, alamat, y_preds, nil, 0,0,0)\r\n            provider_object.set_selected(compared)\r\n            provider_object.set_alamat_prediction(alamat_prediction)\r\n\r\n            provider_list.append(provider_object)\r\n\r\n\r\n    context = {\"list_insurance\":response.get(\"val\"),\"list\":provider_list,\"link_result\":file_location}\r\n\r\n\r\n    return render(request, 'matching/perbandingan.html',context=context)\r\n\r\n\r\ndef tampungan(request):\r\n    link_result = file_location\r\n    if link_result is None:\r\n        link_result = \"-\"\r\n\r\n    context = {\"provider_list\":[],\"link_result\":link_result}\r\n    return render(request, 'matching/perbandingan_basket.html', context=context)\r\n\r\n\r\ndef tampungan_rev(request):\r\n    global provider_liste\r\n    global file_location\r\n    provider_liste = []\r\n\r\n    dfs = None\r\n\r\n    try:\r\n        dfs = pd.read_excel(\"basket_provider.xlsx\")\r\n    except:\r\n        print(\"dataframe not founde\")\r\n\r\n\r\n    provider_name_list = []\r\n    provider_name_predict_list = []\r\n    score_list = []\r\n    # df_dataset = pd.read_excel(\"dataset_excel_copy.xlsx\")\r\n\r\n\r\n\r\n    provider_list = []\r\n    if dfs is not None:\r\n        for index,row in tqdm(dfs.iterrows(),total=dfs.shape[0]):\r\n\r\n\r\n\r\n            provider_name_label = str(row['course_title']).strip().lower()\r\n            alamat = str(row['alamat']).strip().lower()\r\n            concat = provider_name_label+\"#\"+alamat\r\n            concat = concat.replace('&','').replace('.','')\r\n            sample1 = vectorize_text(concat, tfidf_vec1)\r\n            y_preds = loaded_model1.predict(sample1)\r\n            p = loaded_model1.predict_proba(sample1)\r\n            ix = p.argmax(1).item()\r\n            nil = (f'{p[0, ix]:.2}')\r\n            # if(float(nil.strip(\"%\")) < 1.0):\r\n            # y_preds = \"-\"\r\n            provider_name_list.append(provider_name_label)\r\n            provider_name_predict_list.append(y_preds)\r\n            score_list.append(nil)\r\n\r\n            val = (df_non_duplicate['course_titles'].eq(provider_name_label))\r\n            res = df_non_duplicate[val]\r\n            provider_object = ItemPembanding(provider_name_label, alamat, y_preds, nil, 0,0,0)\r\n\r\n            if not res.empty:\r\n                pred = str(y_preds).replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\")\r\n                val_master = (df_non_duplicate['subject'].eq(pred))\r\n                res_master = df_non_duplicate[val_master]\r\n\r\n                al = res_master[\"alamat\"].head(1)\r\n                try:\r\n                    alamat_pred = al.values[0]\r\n                except:\r\n                    print(\"error\")\r\n            elif res.empty:\r\n                alamat_pred = \"-\"\r\n\r\n\r\n            provider_object.set_alamat_prediction(alamat_pred)\r\n\r\n            provider_list.append(provider_object.__dict__)\r\n\r\n    return JsonResponse(provider_list,safe=False)\r\n\r\n\r\ndef hapus_tampungan(request):\r\n    dfs = pd.read_excel(\"basket_provider.xlsx\")\r\n\r\n    if request.method == \"POST\":\r\n        nama = request.POST['nama_provider']\r\n        # nama = json.load(request).get('dats')\r\n        print(nama.upper())\r\n        delete_row = dfs[dfs[\"course_title\"] == nama.upper()].index\r\n        dfs = dfs.drop(delete_row)\r\n        # print(dfs)\r\n        dfs.to_excel('basket_provider.xlsx',index=False)\r\n\r\n\r\n    return HttpResponse(\"OK\")\r\n\r\ndef upload_master(request):\r\n    global provider_liste\r\n    global file_location\r\n    provider_liste = []\r\n    response = requests.get('https://asateknologi.id/api/insuranceall')\r\n    response = response.json()\r\n    dfs = None\r\n\r\n    if request.method == \"POST\":\r\n        file_location = \"media\"+request.POST[\"file_location\"]\r\n\r\n    # elif request.method == \"GET\":\r\n        # file_location=\"media/demo.xlsx\"\r\n\r\n    try:\r\n        dfs = pd.read_excel(file_location)\r\n    except:\r\n        print(\"dataframe not found\")\r\n    provider_list = []\r\n    if dfs is not None:\r\n        for index, row in dfs.iterrows():\r\n            provider_name = row['Provider Name']\r\n            alamat = row['Alamat']\r\n            alamat_prediction = row['Alamat Prediction']\r\n            y_preds = row[\"Prediction\"]\r\n            nil = row[\"Score\"]\r\n            ri = row[\"RI\"]\r\n            rj = row[\"RJ\"]\r\n            provider_object = ItemPembanding(provider_name, alamat, y_preds, nil, 0,ri,rj)\r\n            provider_object.set_alamat_prediction(alamat_prediction)\r\n            provider_list.append(provider_object)\r\n    page = request.GET.get('page', 1)\r\n    paginator = Paginator(provider_list, 10)\r\n    try:\r\n        users = paginator.page(page)\r\n    except PageNotAnInteger:\r\n        users = paginator.page(1)\r\n    except EmptyPage:\r\n        users = paginator.page(paginator.num_pages)\r\n\r\n    context = {\"list_insurance\":response.get(\"val\"),\"list\":provider_list}\r\n\r\n\r\n    return render(request, 'master/bulk_upload.html',context=context)\r\n\r\n\r\ndef list_master(request):\r\n\r\n\r\n    return render(request, 'master/index.html')\r\n\r\n\r\ndef list_master_varian(request):\r\n    return render(request, 'master/index_master_varian.html')\r\n\r\ndef list_master_sinkron(request):\r\n\r\n    return render(request, 'master/sinkron.html')\r\n\r\ndef list_master_process(request):\r\n    master_data_list = []\r\n    dfs  = None\r\n    try:\r\n        dfs = pd.read_excel(\"master_provider.xlsx\")\r\n    except:\r\n        print(\"dataframe not found\")\r\n\r\n    for index, row in dfs.iterrows():\r\n        id = row['ProviderId']\r\n        stateId = row['stateId']\r\n        cityId = row['cityId']\r\n        provider_name_master = str(row['PROVIDER_NAME'])\r\n        address = row['ADDRESS']\r\n        category_1 = row['Category_1']\r\n        category_2 = row['Category_2']\r\n        telephone = row['TEL_NO']\r\n        master_data = MasterData(id, provider_name_master, address, category_1, category_2, telephone, stateId,\r\n                                 cityId)\r\n        master_data_list.append(master_data.__dict__)\r\n    return JsonResponse(master_data_list, safe=False)\r\n\r\n\r\n\r\ndef sinkron_master_process(request):\r\n\r\n    response = requests.get('https://asateknologi.id/api/daftar-rs-1234')\r\n    provider_list = response.json().get(\"val\")\r\n    master_data_list = []\r\n    df = pd.DataFrame()\r\n\r\n    for prov in provider_list:\r\n        id = prov[\"id\"]\r\n        stateId = prov[\"stateId\"]\r\n        cityId = prov[\"CityId\"]\r\n        category_1 = str(prov[\"Category_1\"])\r\n        category_2 = prov[\"Category_2\"]\r\n        telephone = prov[\"TEL_NO\"]\r\n        provider_name_master = prov[\"PROVIDER_NAME\"]\r\n        address = prov[\"ADDRESS\"]\r\n        category = prov[\"Category_1\"]\r\n        master_data = MasterData(id,provider_name_master,address,category_1,category_2,telephone,stateId,cityId)\r\n        master_data_list.append(master_data.__dict__)\r\n        df = df.append(pd.Series(\r\n            {'ProviderId':id,'stateId':stateId,'cityId':cityId,'Category_1':category_1,'Category_2':category_2,'PROVIDER_NAME': provider_name_master, 'ADDRESS':address, 'TEL_NO': telephone},\r\n            name=3))\r\n\r\n\r\n\r\n    df.to_excel(\"master_provider.xlsx\", index=False)\r\n\r\n\r\n    return JsonResponse(master_data_list, safe=False)\r\n\r\ndef download_master(request):\r\n\r\n    file_path = os.getcwd()+\"\\\\master_provider.xlsx\"\r\n    if os.path.exists(file_path):\r\n        with open(file_path, 'rb') as fh:\r\n            response = HttpResponse(fh.read(),\r\n                                    content_type=\"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\")\r\n            response['Content-Disposition'] = 'attachment; filename=master_provider.xlsx'\r\n            return response\r\n    else:\r\n        raise None\r\n    return response\r\n\r\n\r\ndef download_master_varian(request):\r\n\r\n    file_path = os.getcwd()+\"\\\\master_varian_1.xlsx\"\r\n    if os.path.exists(file_path):\r\n        with open(file_path, 'rb') as fh:\r\n            response = HttpResponse(fh.read(),\r\n                                    content_type=\"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\")\r\n            response['Content-Disposition'] = 'attachment; filename=master_varian.xlsx'\r\n            return response\r\n    else:\r\n        raise None\r\n    return response\r\n\r\ndef sinkron_dataset_process(request):\r\n    dff = pd.DataFrame()\r\n\r\n    find = False\r\n    master_data_list = []\r\n    dfs = None\r\n    dfs_varian = None\r\n    try:\r\n        dfs = pd.read_excel(\"master_provider.xlsx\")\r\n        df = cache.get('dataset')\r\n        if df is None:\r\n            df = pd.read_excel(\"dataset_excel_copy.xlsx\")\r\n            cache.set('dataset', df)\r\n        dfs_varian = df.groupby('subject')\r\n    except:\r\n        print(\"dataframe not found\")\r\n    for index, row in dfs.iterrows():\r\n        id = row['ProviderId']\r\n        stateId = row['stateId']\r\n        cityId = row['cityId']\r\n        category_1 = row['Category_1']\r\n        category_2 = row['Category_2']\r\n        provider_name_master = row['PROVIDER_NAME']\r\n        address = row['ADDRESS']\r\n        tel_no = row['TEL_NO']\r\n        master_data = MasterData(id, provider_name_master, address, category_1, category_2, tel_no, stateId, cityId)\r\n        varian_list = []\r\n\r\n        try:\r\n            dfe = dfs_varian.get_group(provider_name_master)\r\n            for index_varian, row_varian in dfe.iterrows():\r\n                varian_list.append(row_varian['course_title'])\r\n                pass\r\n\r\n        except:\r\n            row = pd.Series({'course_title': master_data.nama_provider + \"#\" + master_data.alamat,'alamat': master_data.alamat,'subject': master_data.nama_provider},\r\n                            name=3)\r\n            df = df.append(row)\r\n            df.reset_index(drop=True, inplace=True)\r\n\r\n            continue\r\n\r\n\r\n    df.to_excel(\"dataset_excel_copy.xlsx\",index=False)\r\n    return HttpResponse(\"Tes\")\r\n\r\n\r\n\r\ndef master_varian_process(request):\r\n    dff = pd.DataFrame()\r\n\r\n    find = False\r\n    master_data_list = []\r\n    dfs  = None\r\n    dfs_varian = None\r\n    try:\r\n        dfs = pd.read_excel(\"master_provider.xlsx\")\r\n\r\n        dfs_varian = pd.read_excel(\"dataset_excel_copy.xlsx\").groupby('subject')\r\n    except:\r\n        print(\"dataframe not found\")\r\n\r\n    for index, row in dfs.iterrows():\r\n        id = row['ProviderId']\r\n        stateId = row['stateId']\r\n        cityId = row['cityId']\r\n        category_1 = row['Category_1']\r\n        category_2 = row['Category_2']\r\n        provider_name_master = row['PROVIDER_NAME']\r\n        address = row['ADDRESS']\r\n        tel_no = row['TEL_NO']\r\n        master_data = MasterData(id, provider_name_master, address, category_1, category_2, tel_no, stateId, cityId)\r\n        varian_list = []\r\n\r\n\r\n        try:\r\n            dfe = dfs_varian.get_group(provider_name_master)\r\n            for index_varian, row_varian in dfe.iterrows():\r\n                varian_list.append(row_varian['course_title'])\r\n                pass\r\n\r\n        except:\r\n            continue\r\n\r\n\r\n        master_data.set_varian(varian_list)\r\n\r\n        dff = dff.append(pd.Series(\r\n            {'ProviderId': id,'ProviderType':\"Master\", 'stateId': stateId, 'cityId': cityId, 'Category_1': category_1, 'Category_2': category_2,\r\n             'PROVIDER_NAME': provider_name_master, 'ADDRESS': address, 'TEL_NO': tel_no},\r\n            name=3))\r\n\r\n        for varian in master_data.get_varian():\r\n            dff = dff.append(pd.Series(\r\n                {'ProviderId': id, 'ProviderType': \"Varian\", 'stateId': stateId, 'cityId': cityId,\r\n                 'Category_1': category_1, 'Category_2': category_2,\r\n                 'PROVIDER_NAME': varian, 'ADDRESS': \"-\", 'TEL_NO': \"-\"},\r\n                name=3))\r\n\r\n        master_data_list.append(master_data.__dict__)\r\n\r\n    #\r\n    dff.to_excel(\"master_varian_1.xlsx\", index=False)\r\n\r\n    return JsonResponse(master_data_list, safe=False)\r\n\r\ndef master_varian_list_read(request):\r\n    dff = pd.DataFrame()\r\n\r\n    find = False\r\n    master_data_list = []\r\n    dfs  = None\r\n    try:\r\n        dfs = pd.read_excel(\"master_varian_1.xlsx\")\r\n    except:\r\n        print(\"dataframe not found\")\r\n\r\n    for index, row in dfs.iterrows():\r\n        id = row['ProviderId']\r\n        stateId = row['stateId']\r\n        cityId = row['cityId']\r\n        category_1 = row['Category_1']\r\n        category_2 = row['Category_2']\r\n        provider_name_master = row['PROVIDER_NAME']\r\n        address = row['ADDRESS']\r\n        tel_no = row['TEL_NO']\r\n        master_data = MasterData(id, provider_name_master, address, category_1, category_2, tel_no, stateId, cityId)\r\n        varian_list = []\r\n\r\n        try:\r\n            dfe = dfs_varian.get_group(provider_name_master)\r\n            for index_varian, row_varian in dfe.iterrows():\r\n                varian_list.append(row_varian['course_title'])\r\n                pass\r\n\r\n        except:\r\n            continue\r\n\r\n\r\n        master_data.set_varian(varian_list)\r\n\r\n        dff = dff.append(pd.Series(\r\n            {'ProviderId': id,'ProviderType':\"Master\", 'stateId': stateId, 'cityId': cityId, 'Category_1': category_1, 'Category_2': category_2,\r\n             'PROVIDER_NAME': provider_name_master, 'ADDRESS': address, 'TEL_NO': tel_no},\r\n            name=3))\r\n\r\n        for varian in master_data.get_varian():\r\n            dff = dff.append(pd.Series(\r\n                {'ProviderId': id, 'ProviderType': \"Varian\", 'stateId': stateId, 'cityId': cityId,\r\n                 'Category_1': category_1, 'Category_2': category_2,\r\n                 'PROVIDER_NAME': varian, 'ADDRESS': \"-\", 'TEL_NO': \"-\"},\r\n                name=3))\r\n\r\n        master_data_list.append(master_data.__dict__)\r\n\r\n    #\r\n    dff.to_excel(\"master_varian_1.xlsx\", index=False)\r\n\r\n    return JsonResponse(master_data_list, safe=False)\r\n\r\ndef temporer_store(request):\r\n    global link_result\r\n    if request.method == \"POST\":\r\n        global name\r\n\r\n        post_ide = request.POST[\"post_idew\"]\r\n        alamat = request.POST[\"alamat\"]\r\n        name = post_ide + \"#\"+alamat\r\n        link_result = request.POST[\"link_result\"]\r\n        context = {\"provider_name\": post_ide,\"link_result\":link_result}\r\n\r\n\r\n        if name in provider_liste:\r\n            provider_liste.remove(name)\r\n        else:\r\n            provider_liste.append(name)\r\n    else :\r\n        context = {\"provider_name\":provider_liste,\"link_result\":link_result}\r\n\r\n    # return HttpResponse(context)\r\n    return render(request,'matching/temporer.html',context=context)\r\n\r\n\r\ndef read_link_result_and_delete_provider_name2(nama_provider,link_result):\r\n    global dfs\r\n\r\n    val = (dfs['Provider Name'].str.lower().eq(nama_provider.lower()))\r\n    rese = dfs[val]\r\n\r\n    # if not rese.empty:\r\n    #     print(rese.index.item())\r\n    # print(dfs)\r\n    global deo\r\n    global deoq\r\n    if not rese.empty:\r\n        #\r\n\r\n        try:\r\n            deo = dfs.drop(rese.index.item(),inplace=True)\r\n\r\n            val = (dw['Nama Provider'].str.lower().eq(nama_provider.lower()))\r\n            reseq = dw[val]\r\n            if not reseq.empty:\r\n                deoq = dw.drop(reseq.index.item(),inplace=True)\r\n                # deoq = dw\r\n                # if (nama_provider == \"klinik takenoko sudirman\"):\r\n                #     vae = deoq['Nama Provider'].str.strip().str.lower().eq(\"klinik takenoko sudirman\")\r\n                #     print(deoq[vae])\r\n        except Exception as e:\r\n            for x in rese.index.tolist():\r\n                deo = dfs.drop(x, inplace=True)\r\n\r\n\r\n            pass\r\n\r\ndef read_link_result_and_delete_provider_name(nama_provider):\r\n    dfs = pd.read_excel(link_result)\r\n    val = (dfs['Provider Name'].eq(nama_provider.upper()))\r\n    rese = dfs[val]\r\n    print(\"hapus1 \" + nama_provider, rese)\r\n\r\n    if not rese.empty:\r\n        print(\"hapus \"+nama_provider,rese)\r\n        deo = dfs.drop(rese.index.item())\r\n        deo.to_excel(link_result, sheet_name='Sheet1', index=False)\r\n        dat = Perbandingan.objects.filter(file_location_result__contains=link_result.split(\"/\")[1]).values()\r\n        dw = pd.read_excel(dat[0][\"file_location\"])\r\n        val = (dw['Nama Provider'].eq(nama_provider.upper()))\r\n        reseq = dw[val]\r\n        if not reseq.empty:\r\n            deoq = dw.drop(reseq.index.item())\r\n            deoq.to_excel(dat[0][\"file_location\"], sheet_name='Sheet1', index=False)\r\n\r\n\r\ndef loop_delete(link_result):\r\n    print(\"loop data2 \",link_result)\r\n    df = pd.read_excel(\"Master_Add.xlsx\")\r\n    global dfs\r\n    global deo\r\n    global deoq\r\n    global dw\r\n    deo=None\r\n    deoq=None\r\n    dat = Perbandingan.objects.filter(file_location_result__contains=link_result.split(\"/\")[1]).values()\r\n    dw = pd.read_excel(dat[0][\"file_location\"])\r\n    print(dat[0][\"file_location\"])\r\n    dfs = pd.read_excel(link_result)\r\n    for index, row in tqdm(df.iterrows(),total=df.shape[0]):\r\n        # nama_provider = row['Provider Name']\r\n        nama_provider = row['provider_name']\r\n        read_link_result_and_delete_provider_name2(nama_provider,link_result)\r\n\r\n\r\n\r\n    dfs.to_excel(link_result, sheet_name='Sheet1', index=False)\r\n    # dw.to_excel(dat[0][\"file_location\"], sheet_name='Sheet1', index=False)\r\n\r\n    # if(deo is not None and deoq is not None):\r\n        # deo.to_excel(link_result, sheet_name='Sheet1', index=False)\r\n        # deoq.to_excel(dat[0][\"file_location\"], sheet_name='Sheet1', index=False)\r\n        # deoq.to_excel(\"tay.xlsx\", sheet_name='Sheet1', index=False)\r\n        # deo.to_excel(\"onkar.xlsx\", sheet_name='Sheet1', index=False)\r\n        # dfs.to_excel(\"to.xlsx\",sheet_name='Sheet1',index=False)\r\n\r\n\r\n\r\ndef add_master_store(request):\r\n    if request.method == \"POST\":\r\n        df = pd.read_excel(\"Master_Add.xlsx\")\r\n        post_ide = request.POST[\"post_idew\"]\r\n        nama_provider = post_ide.split(\"#\")[0]\r\n        alamat = post_ide.split(\"#\")[1]\r\n        link_result = request.POST[\"link_result\"]\r\n        val = (df['provider_name'].str.lower().eq(nama_provider.lower()))\r\n        res = df[val]\r\n        # # # kalau kosong alias belum ada nama provider di dalam file master add, maka proses\r\n        if res.empty:\r\n            row = pd.Series({'provider_name': nama_provider, 'alamat': alamat})\r\n            df = df.append(row, ignore_index=True)\r\n            df.to_excel(\"Master_Add.xlsx\",index=False)\r\n\r\n        read_link_result_and_delete_provider_name(nama_provider)\r\n\r\n\r\n\r\n    else:\r\n        return HttpResponse(\"OK\")\r\n\r\n\r\n\r\n    # return HttpResponse(context)\r\n    return HttpResponse(\"OK\")\r\n\r\ndef update_temporer_store(request):\r\n    global name\r\n    global link_result\r\n    if request.method == \"POST\":\r\n        post_ide = request.POST[\"post_idew\"]\r\n        link_result = request.POST[\"link_result\"]\r\n        name = post_ide\r\n        context = {\"provider_name\": post_ide,\"link_result\":link_result}\r\n    else :\r\n        if name in provider_liste:\r\n            provider_liste.remove(name)\r\n        context = {\"provider_name\":provider_liste,\"link_result\":link_result}\r\n    return render(request,'matching/temporer.html',context=context)\r\n\r\n\r\ndef add_to_dataset(request):\r\n    if request.method == \"POST\":\r\n        # OPEN DATASET FILE\r\n        df = cache.get('dataset')\r\n        if df is None:\r\n            df = pd.read_excel(\"dataset_excel_copy.xlsx\")\r\n            cache.set('dataset', df)\r\n\r\n        df_basket = pd.read_excel(\"basket_provider.xlsx\")\r\n\r\n        # SEARCH PROVIDER IN DATASET\r\n        for label_name,provider_name in list(zip(request.POST.getlist('nama_label'),request.POST.getlist('nama_provider'))):\r\n            label_name = label_name.split(\"#\")[0]\r\n            alamat = provider_name.split(\"#\")[1]\r\n\r\n            provider_name = provider_name.split(\"#\")[0]\r\n            for x in range(200):\r\n                try:\r\n                    row = pd.Series({'course_title': provider_name+\"#\"+alamat, 'subject': label_name}, name=3)\r\n                    df = df.append(row,ignore_index=True)\r\n                except:\r\n                    break\r\n            try:\r\n                rowe = pd.Series({'course_title': provider_name,'alamat':alamat}, name=3)\r\n                df_basket = df_basket.append(rowe, ignore_index=True)\r\n            except:\r\n                break\r\n\r\n        # df = df.reset_index(drop=True)\r\n        df_basket.to_excel(\"basket_provider.xlsx\",index=False)\r\n        df.to_excel(\"dataset_excel_copy.xlsx\",index=False)\r\n        # create_model(df)\r\n\r\n        pembanding = models.Perbandingan.objects.all()\r\n        list_pembandinge = pembanding\r\n        list_pembanding = []\r\n        for pembanding in list_pembandinge:\r\n            pembanding.file_location = pembanding.file_location.split(\"media\")[1]\r\n            list_pembanding.append(pembanding)\r\n\r\n        context = {\"list_pembanding\": list_pembanding}\r\n\r\n        return render(request, 'home.html', context)\r\n\r\n    return HttpResponse(\"Marco Polo\")\r\n\r\ndef process_temporer_store(request):\r\n    global link_result\r\n    if request.method == \"POST\":\r\n        link_result  = request.POST[\"link_result\"]\r\n\r\n    dfs = cache.get('dataset')\r\n    if dfs is None:\r\n        dfs = pd.read_excel(\"dataset_excel_copy.xlsx\")\r\n        cache.set('dataset', dfs)\r\n\r\n    # dfa = dfs.drop_duplicates(subset='subject')\r\n    dfz = dfs.dropna(subset=\"alamat\")\r\n    dfa = dfz.drop_duplicates(subset='subject')\r\n    label_list = []\r\n    for index, row in dfa.iterrows():\r\n        provider_name = row['course_title']\r\n        alamat = str(row['alamat'])\r\n        label = row[\"subject\"]\r\n        if label+\"#\"+alamat not in label_list:\r\n            label_list.append(label+\"#\"+alamat)\r\n    print(link_result)\r\n    context = {\"label_list\":label_list,\"list\":provider_liste,\"link_result\":link_result}\r\n    # return HttpResponse(\"Process Temporer\")\r\n    return render(request,'matching/proses_temporer.html',context=context)\r\n\r\n\r\ndef get_label(request):\r\n    dfs = cache.get('dataset')\r\n    if dfs is None:\r\n        dfs = pd.read_excel(\"dataset_excel_copy.xlsx\")\r\n        cache.set('dataset', dfs)\r\n    dfs = dfs.sort_values(by=['subject'], ascending=True)\r\n    dfz = dfs.dropna(subset=\"alamat\")\r\n    dfa = dfz.drop_duplicates(subset='subject')\r\n    label_list = []\r\n    print(dfa.size,dfs.size)\r\n    for index, row in dfa.iterrows():\r\n        provider_name = row['course_title']\r\n        alamat = str(row['alamat'])\r\n        label = row[\"subject\"]\r\n        if label + \"#\" + alamat not in label_list:\r\n            label_list.append(label + \"#\" + alamat)\r\n\r\n    context = {\"label_list\": label_list}\r\n    return JsonResponse(context, safe=False)\r\n\r\n\r\ndef check_header(df):\r\n    header_list = ['Provinsi','Kota','Nama Provider','Alamat']\r\n    df_header_list = list(df.columns.values)\r\n    if df_header_list == header_list:\r\n        return True\r\n    return False\r\n\r\ndef vectorize_text(text,tfidf_vec):\r\n    # text = \"Klinik Ananda\"\r\n    my_vec = tfidf_vec.transform([text])\r\n    return my_vec.toarray()\r\n\r\n\r\ndef pool_process_tampungan(dfs):\r\n    # for df in df_list:\r\n    # dataframe Name and Age columns\r\n    pd.options.display.max_colwidth = None\r\n    provider_name_list = []\r\n    provider_name_predict_list = []\r\n    score_list = []\r\n    provider_object_list = []\r\n\r\n    df_result = pd.DataFrame()\r\n    if dfs is not None:\r\n        for index, row in tqdm(dfs.iterrows(), total=dfs.shape[0]):\r\n\r\n            provider_name_label = str(row['course_title']).strip().lower()\r\n            alamat = str(row['alamat']).strip().lower()\r\n            concat = provider_name_label + \"#\" + alamat\r\n            concat = concat.replace('&', '').replace('.', '')\r\n            sample1 = vectorize_text(concat, tfidf_vec)\r\n            y_preds = loaded_model.predict(sample1)\r\n            p = loaded_model.predict_proba(sample1)\r\n            ix = p.argmax(1).item()\r\n            nil = (f'{p[0, ix]:.2}')\r\n            # if(float(nil.strip(\"%\")) < 1.0):\r\n            # y_preds = \"-\"\r\n            provider_name_list.append(provider_name_label)\r\n            provider_name_predict_list.append(y_preds)\r\n            score_list.append(nil)\r\n\r\n            val = (df_non_duplicate['course_titles'].eq(provider_name_label))\r\n            res = df_non_duplicate[val]\r\n            provider_object = ItemPembanding(provider_name_label, alamat, y_preds, nil, 0, 0, 0)\r\n\r\n            if not res.empty:\r\n                pred = str(y_preds).replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\")\r\n                val_master = (df_non_duplicate['subject'].eq(pred))\r\n                res_master = df_non_duplicate[val_master]\r\n\r\n                al = res_master[\"alamat\"].head(1)\r\n                try:\r\n                    alamat_pred = al.values[0]\r\n                except:\r\n                    print(\"error\")\r\n            elif res.empty:\r\n                alamat_pred = \"-\"\r\n\r\n            provider_object.set_alamat_prediction(alamat_pred)\r\n\r\n            provider_list.append(provider_object.__dict__)\r\n\r\ndef pool_process_df(df):\r\n    # for df in df_list:\r\n    # dataframe Name and Age columns\r\n    pd.options.display.max_colwidth = None\r\n    provider_name_list = []\r\n    provider_name_predict_list = []\r\n    score_list = []\r\n    provider_object_list = []\r\n\r\n\r\n    df_result = pd.DataFrame()\r\n    for row in tqdm(df.itertuples(), total=df.shape[0]):\r\n        new_string = row.nama\r\n        alamat = row.alamat\r\n        ri = row.RI\r\n        rj = row.RJ\r\n\r\n        # replace with df_nama_alamat\r\n        nama_alamat = row.nama_alamat\r\n\r\n        provider_name = new_string\r\n\r\n        # course_title = apotik  klinik kimia farma  cilegon#jl. s.a. tirtayasa no 12\r\n\r\n        # val = (df_non_duplicate['course_title'].str.lower().str.strip().eq(nama_alamat))\r\n        val = (df_non_duplicate['course_title'].eq(nama_alamat))\r\n\r\n        res = df_non_duplicate[val]\r\n\r\n\r\n        provider_name_list.append(provider_name)\r\n        # load the model from disk\r\n        sample1 = vectorize_text(nama_alamat, tfidf_vec1)\r\n        y_preds = loaded_model1.predict(sample1)\r\n        p = loaded_model1.predict_proba(sample1)\r\n        ix = p.argmax(1).item()\r\n        nil = (f'{p[0, ix]:.2}')\r\n\r\n\r\n        provider_name_predict_list.append(y_preds)\r\n        score_list.append(nil)\r\n        provider_object = ItemPembanding(provider_name, alamat, y_preds, nil, 0,ri,rj)\r\n\r\n        if not res.empty:\r\n            pred = str(y_preds).replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\")\r\n            val_master = (df_non_duplicate['subject'].eq(pred))\r\n            res_master = df_non_duplicate[val_master]\r\n            al = res_master[\"alamat\"].head(1)\r\n            try:\r\n                alamat_pred = al.values[0]\r\n                data_append = {\r\n                    \"Provider Name\": provider_name,\r\n                    \"Alamat\": alamat,\r\n                    \"Prediction\": y_preds,\r\n                    \"Alamat Prediction\": alamat_pred,\r\n                    \"Score\": nil,\r\n                    \"Compared\": 1,\r\n                    \"Clean\": new_string,\r\n                    \"ri\": ri,\r\n                    \"rj\": rj\r\n                }\r\n                provider_object.set_alamat_prediction(alamat_pred)\r\n                df1 = pd.DataFrame(data_append)\r\n            except:\r\n                print(\"error\")\r\n\r\n\r\n        elif res.empty:\r\n\r\n            data_append = {\r\n                \"Provider Name\": provider_name,\r\n                \"Alamat\": alamat,\r\n                \"Prediction\": y_preds,\r\n                \"Alamat Prediction\": \"-\",\r\n                \"Score\": nil,\r\n                \"Compared\": 0,\r\n                \"Clean\": new_string,\r\n                \"ri\":ri,\r\n                \"rj\":rj\r\n            }\r\n            provider_object.set_alamat_prediction(\"-\")\r\n            df1 = pd.DataFrame(data_append)\r\n        provider_object_list.append(provider_object)\r\n        df_result = df_result.append(df1, ignore_index=True)\r\n        # Provider_Perbandingan_data = models.Provider_Perbandingan(nama_asuransi=perbandingan_model.nama_asuransi,\r\n        #                                                           perbandingan_id=1,\r\n        #                                                           name=provider_name_label, address=\"-\", selected=0)\r\n        # Provider_Perbandingan_data.save()\r\n\r\n\r\n\r\n    return df_result\r\n\r\ndef pool_handler(df,perbandingan_model):\r\n    print(\"pool handler\")\r\n\r\n    # df_nama = df['Nama Provider'].str.replace('.','').str.replace('&','').str.replace('-','').str.lower().str.strip()\r\n    df_nama = df['Nama Provider']\r\n    # df_alamat = df['Alamat'].str.lower().str.strip()\r\n    df_alamat = df['Alamat']\r\n    df_ri = df['RI']\r\n    df_rj = df['RJ']\r\n    df_nama_alamat = df_nama.map(str) + '#' + df_alamat.map(str)\r\n    df_lengkap = pd.DataFrame(\r\n        {'nama': df_nama, 'alamat': df_alamat, 'RI': df_ri, 'RJ': df_rj, 'nama_alamat': df_nama_alamat})\r\n\r\n    # # # Split dataframe to many\r\n    df_list = cacah_dataframe(df_lengkap)\r\n\r\n\r\n\r\n    # # # Using multiprocess with pool as many as dataframe list\r\n    p = Pool(len(df_list))\r\n    # # # Use Pool Multiprocessing\r\n    x = p.map(pool_process_df,df_list)\r\n\r\n    # # # Declare write\r\n    writer = pd.ExcelWriter('media/' + perbandingan_model.nama_asuransi + \"_result\" + \".xlsx\",\r\n                            engine='xlsxwriter')\r\n\r\n    # # # Save Perbandingan Model\r\n    perbandingan_model.file_location_result = \"/\" + perbandingan_model.nama_asuransi + \"_result.xlsx\"\r\n    perbandingan_model.save()\r\n\r\n    # # # Concat list of dataframe\r\n    full_dfw = pd.concat(list(x),ignore_index=True)\r\n\r\n    # # # Convert the dataframe to an XlsxWriter Excel object.\r\n    full_dfw.to_excel(writer, sheet_name='Sheet1', index=False)\r\n\r\n    # # # Close the Pandas Excel writer and output the Excel file.\r\n    writer.close()\r\n\r\ndef cacah_dataframe(df):\r\n    split_row_each = 800\r\n    start_index = 0\r\n    iteration_count = int(df.shape[0]/split_row_each)\r\n    sisa = df.shape[0]%split_row_each\r\n    sisa_row = iteration_count*split_row_each+sisa\r\n    df_list = []\r\n    for x in range(iteration_count):\r\n        end_index = start_index + split_row_each\r\n        df_new= df.iloc[start_index:end_index]\r\n        start_index = end_index\r\n        # df_list.append([df_new,lr])\r\n        df_list.append(df_new)\r\n    aw = lambda x,y : y if x > 0 else 0\r\n    df_last = df.iloc[start_index:aw(sisa, sisa_row)]\r\n    df_list.append(df_last)\r\n\r\n    return df_list\r\n\r\n\r\n\r\n\r\ndef is_file_with_this_insurance_exists(nama_asuransi):\r\n    mydata = Perbandingan.objects.filter(nama_asuransi__contains=nama_asuransi).order_by('created_at').values()\r\n    return mydata\r\n\r\n\r\ndef create_result_file(dfs,prediction_list):\r\n    df_master = pd.read_excel(\"master_provider.xlsx\")\r\n    id_list = []\r\n    provider_name_list = []\r\n    provider_name_predict_list = []\r\n    score_list = []\r\n\r\n    writerez = pd.ExcelWriter('media/' + perbandingan_model.nama_asuransi + \"_result_id.xlsx\", engine='xlsxwriter')\r\n    for index_master, row_master in df_master.iterrows():\r\n        id = row_master['ProviderId']\r\n        provider_name_master = str(row_master['PROVIDER_NAME'])\r\n        # provider_name_find = \"['\" + provider_name_master + \"']\"\r\n\r\n        # FIND PREDICTION'S FILE PEMBANDING == NAMA MASTER\r\n        # DENGAN ASUMSI PREDICTION DI FILE PEMBANDING SUDAH AKURAT\r\n        val = (dfs['Prediction'].str.lower().eq(provider_name_master.lower()))\r\n\r\n\r\n        res = dfs[val]\r\n        # print(res.empty)\r\n        if not res.empty:\r\n\r\n            value = res[\"Prediction\"].head(1)\r\n            score = res[\"Score\"].head(1)\r\n            id_list.append(id)\r\n            provider_name_list.append(provider_name_master)\r\n            provider_name_predict_list.append(value.values[0])\r\n            score_list.append(score.values[0])\r\n            prediction_id_object = PredictionId(value.values[0], id)\r\n            prediction_list.append(prediction_id_object)\r\n    df = pd.DataFrame(\r\n        {'id': id_list, 'Master Name': provider_name_list, 'Prediction': provider_name_predict_list,\r\n         'Score': score_list})\r\n    # # Convert the dataframe to an XlsxWriter Excel object.\r\n    df.to_excel(writerez, sheet_name='Sheet1', index=False)\r\n    # # Close the Pandas Excel writer and output the Excel file.\r\n    writerez.close()\r\n    return\r\n\r\ndef create_result_file_final(dfs,prediction_list):\r\n    writere = pd.ExcelWriter('media/' + perbandingan_model.nama_asuransi + \"_result_final.xlsx\", engine='xlsxwriter')\r\n\r\n    provider_list = []\r\n    provider_name_list_final = []\r\n    id_list_final = []\r\n    provider_name_predict_list_final = []\r\n    score_list_final = []\r\n    ri_list = []\r\n    rj_list = []\r\n    for index, row in dfs.iterrows():\r\n        provider_name = row['Provider Name']\r\n        y_preds = row[\"Prediction\"]\r\n        nil = row[\"Score\"]\r\n        alamat = row[\"Alamat\"]\r\n        ri = row[\"ri\"]\r\n        rj = row[\"rj\"]\r\n        alamat_pred = row[\"Alamat Prediction\"]\r\n        provider_object = ItemPembanding(provider_name, alamat, y_preds, nil, 0,ri,rj)\r\n        provider_object.set_id_master(\"-\")\r\n        provider_object.set_alamat_prediction(alamat_pred)\r\n\r\n        for preds in prediction_list:\r\n            # COMPARE FILE PEMBANDING  dengan PREDICTION VALUE DENGAN ASUMSI PEMBANDING UDAH BENER\r\n            if preds.prediction == provider_object.get_label_name():\r\n                provider_object.set_id_master(preds.id_master)\r\n\r\n        provider_name_list_final.append(provider_object.get_nama_provider())\r\n        provider_name_predict_list_final.append(provider_object.get_label_name())\r\n        score_list_final.append(provider_object.get_proba_score())\r\n        id_list_final.append(provider_object.get_id_master())\r\n        ri_list.append(provider_object.get_ri())\r\n        rj_list.append(provider_object.get_rj())\r\n\r\n        provider_list.append(provider_object)\r\n\r\n    df = pd.DataFrame(\r\n        {'id_master': id_list_final, 'Provider Name': provider_name_list_final,\r\n         'Prediction': provider_name_predict_list_final,\r\n         'Score': score_list_final,\r\n         'ri':ri_list,\r\n         'rj':rj_list})\r\n    # # Convert the dataframe to an XlsxWriter Excel object.\r\n    df.to_excel(writere, sheet_name='Sheet1', index=False)\r\n    # # Close the Pandas Excel writer and output the Excel file.\r\n    writere.close()\r\n    return provider_list\r\n\r\n\r\ndef update_perbandingan_excel():\r\n    pass\r\n\r\ndef perbandingan_result(request):\r\n    global uploaded_file\r\n    global contexte\r\n    if request.method == 'POST':\r\n        uploaded_file = None\r\n        file_extension = None\r\n        filename = None\r\n        fs = FileSystemStorage()\r\n        if not bool(request.FILES.get('perbandinganModel',False)) :\r\n            uploaded_file = request.POST['perbandinganModelFile']\r\n            file_extension = pathlib.Path(\"media/\"+uploaded_file).suffix\r\n            file_content = pathlib.Path(\"media/\"+uploaded_file)\r\n            filename = file_content\r\n\r\n        else:\r\n            uploaded_file = request.FILES['perbandinganModel']\r\n            file_extension = pathlib.Path(\"media/\" + uploaded_file.name).suffix\r\n            filename = fs.save(uploaded_file.name, uploaded_file)\r\n        menu_insurance = request.POST['insurance_option']\r\n\r\n        if file_extension != \".xlsx\":\r\n            return HttpResponse(\"Extension / Format tidak diizinkan\")\r\n        uploaded_file_path = fs.path(filename)\r\n\r\n        # shutil.copyfile(uploaded_file.name, os.getcwd())\r\n\r\n        insurance_data = is_file_with_this_insurance_exists(menu_insurance)\r\n        global perbandingan_model\r\n\r\n        if not insurance_data:\r\n            perbandingan_model = models.Perbandingan(nama_asuransi=menu_insurance, match_percentage=0,\r\n                                                     status_finish=\"PROCESSING\", file_location=uploaded_file_path)\r\n        else:\r\n            perbandingan_model = Perbandingan.objects.get(pk=insurance_data[0][\"id\"])\r\n\r\n        dfe = pd.read_excel(uploaded_file)\r\n        pembersih = Pembersih(dfe)\r\n        df = pembersih._return_df()\r\n        pool_handler(df,perbandingan_model)\r\n\r\n\r\n        dfs = pd.read_excel(\"media/\"+perbandingan_model.file_location_result)\r\n        prediction_list = []\r\n\r\n        create_result_file(dfs,prediction_list)\r\n        provider_list = create_result_file_final(dfs,prediction_list)\r\n        print(perbandingan_model.file_location_result)\r\n        contexte = {\"list\":provider_list,\"link_result\":\"media/\"+perbandingan_model.file_location_result}\r\n        return render(request, 'matching/perbandingan.html', context=contexte)\r\n\r\n    return render(request, 'matching/perbandingan.html',context=contexte)\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/nlp_provider/views.py b/nlp_provider/views.py
--- a/nlp_provider/views.py	(revision fc9698fd5cf49d7a238f0a8c1d2cd7aa8832cb2d)
+++ b/nlp_provider/views.py	(date 1677061764548)
@@ -27,21 +27,21 @@
 django.setup()
 from model import models
 from model.views import create_model
-from .utils import ItemPembanding, Prediction, MasterData, PredictionId, Pembersih
+from .utils import ItemPembanding, Prediction, MasterData, PredictionId, Pembersih, FilePembandingAsuransi, FileSystem
 from model.models import Provider_Model, Perbandingan, Provider_Perbandingan
 from tqdm import tqdm
 from django.core.cache import cache
 # from .forms import UploadFileForm
 # Create your views here.
-df_dataset = cache.get('dataset')
-if df_dataset is None:
-    df_dataset = pd.read_excel("dataset_excel_copy.xlsx")
-    cache.set('dataset', df_dataset)
-
-new_course_title = df_dataset['course_title'].str.lower().str.split("#", n=1, expand=True)
-df_dataset["course_titles"] = new_course_title[0]
-p = Pembersih((df_dataset.drop_duplicates(['course_title'], keep='first')))
-df_non_duplicate = p._return_df()
+# df_dataset = cache.get('dataset')
+# if df_dataset is None:
+#     df_dataset = pd.read_excel("dataset_excel_copy.xlsx")
+#     cache.set('dataset', df_dataset)
+#
+# new_course_title = df_dataset['course_title'].str.lower().str.split("#", n=1, expand=True)
+# df_dataset["course_titles"] = new_course_title[0]
+# p = Pembersih((df_dataset.drop_duplicates(['course_title'], keep='first')))
+# df_non_duplicate = p._return_df()
 filename = 'tfidf_vec.pickle'
 tfidf_vec1 = pickle.load(open(filename, 'rb'))
 filename = 'finalized_model.sav'
@@ -851,54 +851,6 @@
     return my_vec.toarray()
 
 
-def pool_process_tampungan(dfs):
-    # for df in df_list:
-    # dataframe Name and Age columns
-    pd.options.display.max_colwidth = None
-    provider_name_list = []
-    provider_name_predict_list = []
-    score_list = []
-    provider_object_list = []
-
-    df_result = pd.DataFrame()
-    if dfs is not None:
-        for index, row in tqdm(dfs.iterrows(), total=dfs.shape[0]):
-
-            provider_name_label = str(row['course_title']).strip().lower()
-            alamat = str(row['alamat']).strip().lower()
-            concat = provider_name_label + "#" + alamat
-            concat = concat.replace('&', '').replace('.', '')
-            sample1 = vectorize_text(concat, tfidf_vec)
-            y_preds = loaded_model.predict(sample1)
-            p = loaded_model.predict_proba(sample1)
-            ix = p.argmax(1).item()
-            nil = (f'{p[0, ix]:.2}')
-            # if(float(nil.strip("%")) < 1.0):
-            # y_preds = "-"
-            provider_name_list.append(provider_name_label)
-            provider_name_predict_list.append(y_preds)
-            score_list.append(nil)
-
-            val = (df_non_duplicate['course_titles'].eq(provider_name_label))
-            res = df_non_duplicate[val]
-            provider_object = ItemPembanding(provider_name_label, alamat, y_preds, nil, 0, 0, 0)
-
-            if not res.empty:
-                pred = str(y_preds).replace("[", "").replace("]", "").replace("'", "")
-                val_master = (df_non_duplicate['subject'].eq(pred))
-                res_master = df_non_duplicate[val_master]
-
-                al = res_master["alamat"].head(1)
-                try:
-                    alamat_pred = al.values[0]
-                except:
-                    print("error")
-            elif res.empty:
-                alamat_pred = "-"
-
-            provider_object.set_alamat_prediction(alamat_pred)
-
-            provider_list.append(provider_object.__dict__)
 
 def pool_process_df(df):
     # for df in df_list:
@@ -1155,38 +1107,43 @@
     global uploaded_file
     global contexte
     if request.method == 'POST':
-        uploaded_file = None
-        file_extension = None
-        filename = None
-        fs = FileSystemStorage()
+        filePembandingAsuransi = FilePembandingAsuransi()
+        fileSystem = FileSystem(filePembandingAsuransi)
+
+        # GET INSURANCE NAME
+        filePembandingAsuransi.set_nama_asuransi(request.POST['insurance_option'])
+
         if not bool(request.FILES.get('perbandinganModel',False)) :
-            uploaded_file = request.POST['perbandinganModelFile']
-            file_extension = pathlib.Path("media/"+uploaded_file).suffix
-            file_content = pathlib.Path("media/"+uploaded_file)
-            filename = file_content
+            filePembandingAsuransi.set_uploaded_file(request.POST['perbandinganModelFile'])
+            filePembandingAsuransi.set_extension_file_pembanding()
+            filePembandingAsuransi.set_nama_file_pembanding()
 
         else:
-            uploaded_file = request.FILES['perbandinganModel']
-            file_extension = pathlib.Path("media/" + uploaded_file.name).suffix
-            filename = fs.save(uploaded_file.name, uploaded_file)
-        menu_insurance = request.POST['insurance_option']
+            filePembandingAsuransi.set_uploaded_file(request.FILES['perbandinganModel'])
+            filePembandingAsuransi.set_nama_file_pembanding()
+            filePembandingAsuransi.set_extension_file_pembanding()
+            fileSystem.save_file()
 
-        if file_extension != ".xlsx":
+
+        # JIKA FILE EKSTENSI TIDAK DIIZINKAN RETURN FALSE
+        if filePembandingAsuransi.get_extension_file_pembanding() != ".xlsx":
             return HttpResponse("Extension / Format tidak diizinkan")
-        uploaded_file_path = fs.path(filename)
+
+        uploaded_file_path = fileSystem.get_path()
 
-        # shutil.copyfile(uploaded_file.name, os.getcwd())
 
-        insurance_data = is_file_with_this_insurance_exists(menu_insurance)
+        insurance_data = is_file_with_this_insurance_exists(filePembandingAsuransi.get_nama_asuransi())
+
         global perbandingan_model
 
         if not insurance_data:
-            perbandingan_model = models.Perbandingan(nama_asuransi=menu_insurance, match_percentage=0,
+            perbandingan_model = models.Perbandingan(nama_asuransi=filePembandingAsuransi.get_nama_asuransi(), match_percentage=0,
                                                      status_finish="PROCESSING", file_location=uploaded_file_path)
         else:
             perbandingan_model = Perbandingan.objects.get(pk=insurance_data[0]["id"])
 
-        dfe = pd.read_excel(uploaded_file)
+        dfe = pd.read_excel("media/bri_7Azj1ZO_NKr8slo.xlsx")
+        print(dfe)
         pembersih = Pembersih(dfe)
         df = pembersih._return_df()
         pool_handler(df,perbandingan_model)
Index: nlp_provider/utils.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>class ItemPembanding:\r\n    def __init__(self,nama_provider,alamat,label_name,proba_score,count_label,ri,rj):\r\n        self.nama_provider = str(nama_provider).strip()\r\n        self.alamat = str(alamat).strip()\r\n        self.label_name = str(label_name).strip()\r\n        self.proba_score = str(proba_score).strip()\r\n        self.count_label_name = str(count_label).strip()\r\n        self.ri = str(ri).strip()\r\n        self.rj = str(rj).strip()\r\n\r\n    def get_ri(self):\r\n        if(self.ri == \"Y\"):\r\n            return \"1\"\r\n        elif self.ri == \"N\":\r\n            return \"0\"\r\n\r\n    def get_rj(self):\r\n        if (self.rj == \"Y\"):\r\n            return \"1\"\r\n        elif self.rj == \"N\":\r\n            return \"0\"\r\n\r\n\r\n    def set_alamat_prediction(self,alamat_prediction):\r\n        self.alamat_prediction = str(alamat_prediction)\r\n\r\n    def get_alamat_prediction(self):\r\n        return self.alamat_prediction\r\n\r\n    def set_mapped_times(self,times):\r\n        self.mapped_times = times\r\n\r\n    def set_id_master(self,id):\r\n        self.id_master = id\r\n\r\n\r\n    def get_id_master(self):\r\n        return self.id_master\r\n\r\n    def get_mapped_times(self):\r\n        return self.mapped_times\r\n\r\n    def set_count_label_name(self,count):\r\n        self.count_label_name = count\r\n\r\n    def set_selected(self,selected):\r\n        self.selected = selected\r\n\r\n    def set_nama_asuransi(self,nama_asuransi):\r\n        self.nama_asuransi = nama_asuransi\r\n\r\n\r\n    def get_count_label_name(self):\r\n        return self.count_label_name\r\n\r\n    def get_nama_asuransi(self):\r\n        return self.nama_asuransi\r\n\r\n    def get_nama_provider(self):\r\n        return self.nama_provider\r\n\r\n    def get_alamat(self):\r\n        return self.alamat\r\n\r\n    def get_label_name(self):\r\n        return self.label_name\r\n\r\n    def get_proba_score(self):\r\n        return self.proba_score\r\n\r\n    def get_selected(self):\r\n        return self.selected\r\n\r\nclass Prediction:\r\n    def __init__(self,nama_provider,alamat):\r\n        self.nama_provider = nama_provider\r\n        self.alamat = alamat\r\n\r\n    def get_nama_provider(self):\r\n        return self.nama_provider\r\n\r\n    def get_count_refer(self):\r\n        return self.count_refer\r\n\r\n    def get_alamat(self):\r\n        return self.alamat\r\n\r\n    def set_count_refer(self,count_refer):\r\n        self.count_refer = count_refer\r\n\r\n\r\n\r\nclass MasterData:\r\n    def __init__(self,id,nama_provider,alamat,category_1,category_2,phone,state_id,city_id):\r\n        self.id = str(id)\r\n        self.state_id = str(state_id)\r\n        self.city_id = str(city_id)\r\n        self.category_1 = str(category_1)\r\n        self.category_2 = str(category_2)\r\n        self.nama_provider = str(nama_provider).replace(\"_x000D_\",\"\")\r\n        self.alamat = str(alamat).replace(\"_x000D_\",\"\")\r\n        self.phone = str(phone)\r\n\r\n\r\n    def set_varian(self,varian):\r\n        self.varian = varian\r\n\r\n\r\n    def get_varian(self):\r\n        return self.varian\r\n\r\n\r\nclass PredictionId:\r\n    def __init__(self,prediction,id_master):\r\n        self.prediction = prediction\r\n        self.id_master = id_master\r\n\r\nclass FilePembandingAsuransi:\r\n    def __init__(self,nama_asuransi,match_percentage,status_finish,file_location,file_location_result,created_at):\r\n        self.nama_asuransi = nama_asuransi\r\n        self.match_percentage = match_percentage\r\n        self.status_finish = status_finish\r\n        self.file_location = file_location\r\n        self.file_location_result = file_location_result\r\n        self.created_at = created_at\r\n\r\n    def set_provider_list(self,provider_data_list):\r\n        self.provider_data_list = provider_data_list\r\n\r\n    def get_provider_list(self):\r\n        return self.provider_data_list\r\n\r\n\r\n\r\nclass Pembersih:\r\n    def __init__(self,df):\r\n        self.df1 = df\r\n        self._rubah_dataframe_astype_str()\r\n        self._hilangkan_tanda_baca()\r\n        self._kecilkan_tulisan()\r\n    def _kecilkan_tulisan(self):\r\n        self.df4 = self.df3.applymap(str.lower)\r\n\r\n    def _hilangkan_tanda_baca(self):\r\n        self.df3 = self.df2.replace(to_replace=['\\.','\\&'],value='',inplace=False,regex=True)\r\n\r\n    def _rubah_dataframe_astype_str(self):\r\n        self.df2 = self.df1.astype(str)\r\n\r\n    def _return_astype_str(self):\r\n        return self.df\r\n\r\n    def _return_df(self):\r\n        print(self.df4)\r\n        return self.df4\r\n\r\n\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/nlp_provider/utils.py b/nlp_provider/utils.py
--- a/nlp_provider/utils.py	(revision fc9698fd5cf49d7a238f0a8c1d2cd7aa8832cb2d)
+++ b/nlp_provider/utils.py	(date 1677061728090)
@@ -1,3 +1,8 @@
+import pathlib
+
+from django.core.files.storage import FileSystemStorage
+
+
 class ItemPembanding:
     def __init__(self,nama_provider,alamat,label_name,proba_score,count_label,ri,rj):
         self.nama_provider = str(nama_provider).strip()
@@ -116,13 +121,14 @@
         self.id_master = id_master
 
 class FilePembandingAsuransi:
-    def __init__(self,nama_asuransi,match_percentage,status_finish,file_location,file_location_result,created_at):
-        self.nama_asuransi = nama_asuransi
-        self.match_percentage = match_percentage
-        self.status_finish = status_finish
-        self.file_location = file_location
-        self.file_location_result = file_location_result
-        self.created_at = created_at
+    def __init__(self):
+        pass
+        # self.nama_asuransi = nama_asuransi
+        # self.match_percentage = match_percentage
+        # self.status_finish = status_finish
+        # self.file_location = file_location
+        # self.file_location_result = file_location_result
+        # self.created_at = created_at
 
     def set_provider_list(self,provider_data_list):
         self.provider_data_list = provider_data_list
@@ -130,6 +136,59 @@
     def get_provider_list(self):
         return self.provider_data_list
 
+    def set_nama_file_pembanding(self):
+        print("uploaded file "+self.uploaded_file.name)
+        # self.nama_file = pathlib.Path("media/"+self.uploaded_file.name)
+        self.nama_file = self.uploaded_file.name
+        self.set_lokasi_file_pembanding()
+
+    def set_lokasi_file_pembanding(self):
+        self.lokasi_file_pembanding = "media/"+self.uploaded_file.name
+
+
+    def set_extension_file_pembanding(self):
+        try:
+            self.file_extension = pathlib.Path("media/"+self.uploaded_file).suffix
+        except:
+            self.file_extension = pathlib.Path("media/" +self.uploaded_file.name).suffix
+    def set_nama_asuransi(self,nama_asuransi):
+        self.nama_asuransi = nama_asuransi
+
+    def set_uploaded_file(self,uploaded_file):
+        self.uploaded_file = uploaded_file
+
+
+    def get_lokasi_pembanding(self):
+        print("lokasi file "+self.lokasi_file_pembanding)
+        return self.lokasi_file_pembanding
+    def get_uploaded_file(self):
+        print("uploaded file "+self.uploaded_file.name)
+        return self.uploaded_file
+
+    def get_nama_asuransi(self):
+        return self.nama_asuransi
+
+    def get_extension_file_pembanding(self):
+        print("Ekstensi adalah "+self.file_extension)
+        return self.file_extension
+
+    def get_nama_file_pembanding(self):
+        return self.nama_file
+
+
+class FileSystem:
+    def __init__(self,filePembandingAsuransi):
+        self.file = filePembandingAsuransi
+        self.fst = FileSystemStorage()
+
+    def get_path(self):
+        return self.fst.path(self.file.get_nama_file_pembanding())
+
+
+    def save_file(self):
+        self.fst.save(self.file.get_uploaded_file().name,self.file.get_uploaded_file())
+        print("nama file "+self.file.get_nama_file_pembanding())
+
 
 
 class Pembersih:
Index: model/views.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import multiprocessing\r\nimport queue\r\nimport re\r\nimport shutil\r\nimport string\r\nimport time\r\nfrom multiprocessing import Pool, Process, Queue\r\n\r\nimport numpy as np\r\nfrom django.core.files.storage import FileSystemStorage\r\nfrom django.http import HttpResponse\r\nfrom django.shortcuts import render\r\nimport pandas as pd\r\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier, SGDRegressor\r\nimport django\r\n\r\nfrom nlp_provider.utils import Pembersih\r\n\r\ndjango.setup()\r\nimport pickle\r\nfrom sklearn.feature_extraction.text import TfidfVectorizer\r\n\r\nimport model.models\r\nfrom . import views, models\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.feature_extraction.text import TfidfVectorizer\r\nfrom sklearn.model_selection import train_test_split\r\n\r\n\r\n# Create your views here.\r\ndef index(request):\r\n    list_model = models.Provider_Model.objects.all()\r\n    context_all = {'list_model':list_model}\r\n    return render(request,\"model/model.html\",context=context_all)\r\n\r\n\r\ndef upload_file(request):\r\n    if request.method == 'POST':\r\n        uploaded_file = request.FILES['documentModel']\r\n        print(uploaded_file.name)\r\n        fs = FileSystemStorage()\r\n        fs.save(uploaded_file.name,uploaded_file)\r\n        # Load Dataset\r\n        df = pd.read_excel(uploaded_file)\r\n        create_model(df)\r\n\r\n\r\n    list_model = models.Provider_Model.objects.all()\r\n    context_all = {'list_model':list_model}\r\n\r\n    return render(request, 'model/model.html',context=context_all)\r\n\r\ndef work_log(work_data):\r\n    df = work_data[0]\r\n    lr_model = work_data[1]\r\n    print(df)\r\n    # print(\" Process %s waiting %s seconds\" % (work_data[0].head(), work_data[1]))\r\n    # print(\" Process %s Finished.\" % work_data[0].head())\r\n    #\r\n    df['clean_course_title'] = df['course_title'].astype(str)\r\n    # df['clean_course_title'] = df['clean_course_title'].apply(lambda x: ' '.join([ps.stem(word) for word in x.split() if word not in set(all_stopwords)]))\r\n    df['clean_course_title'] = df['clean_course_title'].fillna('').astype(str).replace('', np.nan, regex=False)\r\n    new_string = df['clean_course_title'].str.replace('.','')\r\n    new_string = new_string.str.lower()\r\n    new_string = new_string.str.replace('&','')\r\n    df['clean_course_title'] = new_string\r\n    print(\"Improt Tfidf\")\r\n    Xfeatures = df['clean_course_title']\r\n    ylabels = df['subject']\r\n    tfidf_vec = TfidfVectorizer()\r\n    X = tfidf_vec.fit_transform(Xfeatures.values.astype('U'))\r\n\r\n    print(\"Split Dataset\")\r\n    # # Split our dataset\r\n    x_train,x_test,y_train,y_test = train_test_split(X,ylabels,test_size=0.2,random_state=42)\r\n    #\r\n    #\r\n    # # Build Model\r\n    print(\"Fit Model\")\r\n    lr_model.fit(x_train,y_train)\r\n    #\r\n    print(lr_model.score(x_test,y_test))\r\n\r\n    print(\"Open Pickle\")\r\n    pickle.dump(tfidf_vec, open('tfidf_vec.pickle', 'wb'))\r\n\r\n    # # save the model to disk\r\n    print(\"Save model to disk\")\r\n    filename = 'finalized_model.sav'\r\n    pickle.dump(lr_model, open(filename, 'wb'))\r\n\r\n    print(\"Save Model\")\r\n    model_create = models.Provider_Model(model_name=filename, accuracy_score=str(lr_model.score(x_test,y_test)), model_location='drive C')\r\n    model_create.save()\r\n\r\n    # load the model from disk\r\n    print(\"Load Model\")\r\n    loaded_model = pickle.load(open(filename, 'rb'))\r\n    result = loaded_model.score(x_test, y_test)\r\n    print(result)\r\n\r\n    ex = \"RS. CIPUTRA CITRA GARDEN CITY\"\r\n\r\n\r\n    def vectorize_text(text):\r\n        my_vec = tfidf_vec.transform([text])\r\n        return my_vec.toarray()\r\n\r\n    vectorize_text(ex)\r\n    # df.to_excel('wew.xlsx')\r\n    sample1 = vectorize_text(ex)\r\n    pred = lr_model.predict(sample1)\r\n\r\n    print(pred)\r\n    print(\"Finish Creating Model\")\r\n\r\n\r\ndef work_log_process(df_to_accomplish,lr_model):\r\n    df = df_to_accomplish\r\n    # print(\" Process %s waiting %s seconds\" % (work_data[0].head(), work_data[1]))\r\n    # print(\" Process %s Finished.\" % work_data[0].head())\r\n    #\r\n    df['clean_course_title'] = df['course_title'].astype(str)\r\n    # df['clean_course_title'] = df['clean_course_title'].apply(lambda x: ' '.join([ps.stem(word) for word in x.split() if word not in set(all_stopwords)]))\r\n    df['clean_course_title'] = df['clean_course_title'].fillna('').astype(str).replace('', np.nan, regex=False)\r\n    new_string = df['clean_course_title'].str.replace('.', '')\r\n    new_string = new_string.str.lower()\r\n    new_string = new_string.str.replace('&', '')\r\n    df['clean_course_title'] = new_string\r\n    print(\"Improt Tfidf\")\r\n    Xfeatures = df['clean_course_title']\r\n    ylabels = df['subject']\r\n    tfidf_vec = TfidfVectorizer()\r\n    X = tfidf_vec.fit_transform(Xfeatures.values.astype('U'))\r\n\r\n    print(\"Split Dataset\")\r\n    # # Split our dataset\r\n    x_train, x_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.2, random_state=42)\r\n    #\r\n    #\r\n    filename = 'finalized_model.sav'\r\n    # open saved model\r\n    loaded_model = pickle.load(open(filename, 'rb'))\r\n    result = loaded_model.score(x_test, y_test)\r\n    # print(result)\r\n    # # Build Model\r\n    # print(\"Fit Model\")\r\n    # lr_model.fit(x_train, y_train)\r\n    #\r\n    # #\r\n    # print(lr_model.score(x_test, y_test))\r\n    #\r\n    # print(\"Open Pickle\")\r\n    # pickle.dump(tfidf_vec, open('tfidf_vec.pickle', 'wb'))\r\n    #\r\n    # # # save the model to disk\r\n    # print(\"Save model to disk\")\r\n    # pickle.dump(lr_model, open(filename, 'wb'))\r\n    #\r\n    # print(\"Save Model\")\r\n    # model_create = models.Provider_Model(model_name=filename,\r\n    #                                      accuracy_score=str(lr_model.score(x_test, y_test)),\r\n    #                                      model_location='drive C')\r\n    # model_create.save()\r\n    #\r\n    # # load the model from disk\r\n    # print(\"Load Model\")\r\n    # loaded_model = pickle.load(open(filename, 'rb'))\r\n    # result = loaded_model.score(x_test, y_test)\r\n    # print(result)\r\n    #\r\n    # ex = \"RS. CIPUTRA CITRA GARDEN CITY\"\r\n    #\r\n    # def vectorize_text(text):\r\n    #     my_vec = tfidf_vec.transform([text])\r\n    #     return my_vec.toarray()\r\n    #\r\n    # vectorize_text(ex)\r\n    # # df.to_excel('wew.xlsx')\r\n    # sample1 = vectorize_text(ex)\r\n    # pred = lr_model.predict(sample1)\r\n    #\r\n    # print(pred)\r\n    # print(\"Finish Creating Model\")\r\n    return True\r\n\r\ndef vectorize_text(text,tfidf_vec):\r\n        my_vec = tfidf_vec.transform([text])\r\n        return my_vec.toarray()\r\n\r\ndef biasa_aja(df,lr_model,cnt):\r\n    # # Build Model\r\n    filename = 'finalized_model_'+str(cnt)+'.sav'\r\n    pickle_name = 'tfidf_vec.pickle'\r\n    df['clean_course_title'] = df['course_title'].astype(str)\r\n    # df['clean_course_title'] = df['clean_course_title'].apply(lambda x: ' '.join([ps.stem(word) for word in x.split() if word not in set(all_stopwords)]))\r\n    df['clean_course_title'] = df['clean_course_title'].fillna('').astype(str).replace('', np.nan, regex=False)\r\n    new_string = df['clean_course_title'].str.replace('.', '')\r\n    new_string = new_string.str.lower()\r\n    new_string = new_string.str.replace('&', '')\r\n    df['clean_course_title'] = new_string\r\n    print(df.shape)\r\n    print(\"Improt Tfidf\")\r\n    Xfeatures = df['clean_course_title']\r\n    ylabels = df['subject']\r\n\r\n    # buka file\r\n    try:\r\n        loaded_model = pickle.load(open(filename, 'rb'))\r\n        tfidf_vec = pickle.load(open(pickle_name,'rb'))\r\n        lr_model = loaded_model\r\n\r\n    except:\r\n        print(\"gagal\")\r\n        tfidf_vec = TfidfVectorizer()\r\n\r\n    X = tfidf_vec.fit_transform(Xfeatures.values.astype('U'))\r\n    print(\"Split Dataset\")\r\n    # # Split our dataset\r\n    x_train, x_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.2, random_state=42)\r\n\r\n    lr_model.fit(x_train, y_train)\r\n\r\n    print(\"Open Pickle\")\r\n    pickle.dump(tfidf_vec, open(pickle_name, 'wb'))\r\n\r\n    # # save the model to disk\r\n    print(\"Save model to disk\")\r\n    filenamez = \"finalized_model\"+\"_\"+str(cnt)+\".sav\"\r\n    pickle.dump(lr_model, open(filenamez, 'wb'))\r\n\r\n    print(\"Save Model\")\r\n    model_create = models.Provider_Model(model_name=filenamez,\r\n                                         accuracy_score=str(lr_model.score(x_test, y_test)),\r\n                                         model_location='drive C')\r\n    model_create.save()\r\n\r\n    # load the model from disk\r\n    print(\"Load Model\")\r\n    loaded_model = pickle.load(open(filename, 'rb'))\r\n    result = loaded_model.score(x_test, y_test)\r\n    print(result)\r\n    ex = \"RS. CIPUTRA CITRA GARDEN CITY\"\r\n    vectorize_text(ex,tfidf_vec)\r\n    # df.to_excel('wew.xlsx')\r\n    sample1 = vectorize_text(ex,tfidf_vec)\r\n    pred = lr_model.predict(sample1)\r\n\r\n    print(pred)\r\n    print(\"Finish Creating Model\")\r\n    print(\"Fit Model\")\r\n\r\n\r\ndef pool_handler(df_list):\r\n    p = Pool(2)\r\n    p.map(work_log,df_list)\r\n\r\ndef queue_handler(df_list,lr_model):\r\n    cnt = 1\r\n    que = Queue()\r\n    for df in df_list:\r\n        que.put(df)\r\n\r\n    while not que.empty():\r\n        df = que.get()\r\n        print(df)\r\n        df['clean_course_title'] = df['course_title'].astype(str)\r\n        # df['clean_course_title'] = df['clean_course_title'].apply(lambda x: ' '.join([ps.stem(word) for word in x.split() if word not in set(all_stopwords)]))\r\n        df['clean_course_title'] = df['clean_course_title'].fillna('').astype(str).replace('', np.nan, regex=False)\r\n        new_string = df['clean_course_title'].str.replace('.', '')\r\n        new_string = new_string.str.lower()\r\n        new_string = new_string.str.replace('&', '')\r\n        df['clean_course_title'] = new_string\r\n        print(\"Improt Tfidf\")\r\n        Xfeatures = df['clean_course_title']\r\n        ylabels = df['subject']\r\n        tfidf_vec = TfidfVectorizer()\r\n        X = tfidf_vec.fit_transform(Xfeatures.values.astype('U'))\r\n\r\n        print(\"Split Dataset\")\r\n        # # Split our dataset\r\n        x_train, x_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.2, random_state=42)\r\n        #\r\n        #\r\n        # # Build Model\r\n        print(\"Fit Model\")\r\n        lr_model.fit(x_train, y_train)\r\n        #\r\n        print(lr_model.score(x_test, y_test))\r\n\r\n        print(\"Open Pickle\")\r\n        pickle.dump(tfidf_vec, open('tfidf_vec.pickle', 'wb'))\r\n\r\n        # # save the model to disk\r\n        print(\"Save model to disk\")\r\n        filename = 'finalized_model.sav'\r\n        pickle.dump(lr_model, open(filename, 'wb'))\r\n\r\n        print(\"Save Model\")\r\n        model_create = models.Provider_Model(model_name=filename,\r\n                                             accuracy_score=str(lr_model.score(x_test, y_test)),\r\n                                             model_location='drive C')\r\n        model_create.save()\r\n\r\n        # load the model from disk\r\n        print(\"Load Model\")\r\n        loaded_model = pickle.load(open(filename, 'rb'))\r\n        result = loaded_model.score(x_test, y_test)\r\n        print(result)\r\n\r\n        ex = \"RS. CIPUTRA CITRA GARDEN CITY\"\r\n\r\n        def vectorize_text(text):\r\n            my_vec = tfidf_vec.transform([text])\r\n            return my_vec.toarray()\r\n\r\n        vectorize_text(ex)\r\n        # df.to_excel('wew.xlsx')\r\n        sample1 = vectorize_text(ex)\r\n        pred = lr_model.predict(sample1)\r\n\r\n        print(pred)\r\n        print(\"Finish Creating Model\")\r\n\r\ndef process_handler(df_list,lr_model):\r\n    number_of_process = len(df_list)\r\n    df_to_accomplish = Queue()\r\n    for df in df_list:\r\n        df_to_accomplish.put(df)\r\n\r\n    processes = []\r\n    for x in range(number_of_process):\r\n        p = Process(target=work_log_process,args=(df_list[x],lr_model))\r\n        p.start()\r\n        processes.append(p)\r\n\r\n    for p in processes:\r\n        p.join()\r\ndef cacah_dataframe(df,lr):\r\n    split_row_each = 5000\r\n    start_index = 0\r\n    iteration_count = int(df.shape[0]/split_row_each)\r\n    sisa = df.shape[0]%split_row_each\r\n    sisa_row = iteration_count*split_row_each+sisa\r\n    df_list = []\r\n    for x in range(iteration_count):\r\n        end_index = start_index + split_row_each\r\n        df_new= df.iloc[start_index:end_index]\r\n        start_index = end_index\r\n        # df_list.append([df_new,lr])\r\n        df_list.append(df_new)\r\n    aw = lambda x,y : y if x > 0 else 0\r\n    df_last = df.iloc[start_index:aw(sisa, sisa_row)]\r\n    # df_list.append([df_last,lr])\r\n    df_list.append(df_last)\r\n    # pool_handler(df_list)\r\n    # queue_handler(df_list,lr)\r\n    # process_handler(df_list,lr)\r\n    cnt = 0\r\n    for df in df_list:\r\n        biasa_aja(df,lr,cnt)\r\n        cnt+=1\r\n\r\n\r\n\r\n\r\ndef process_partially(df):\r\n    print(\"Number of cpu : \",multiprocessing.cpu_count())\r\n\r\n    df['clean_course_title'] = df['course_title'].astype(str)\r\n    # df['clean_course_title'] = df['clean_course_title'].apply(lambda x: ' '.join([ps.stem(word) for word in x.split() if word not in set(all_stopwords)]))\r\n    df['clean_course_title'] = df['clean_course_title'].fillna('').astype(str).replace('', np.nan, regex=False)\r\n    new_string = df['clean_course_title'].str.replace('.', '')\r\n    new_string = new_string.str.lower()\r\n    new_string = new_string.str.replace('&', '')\r\n    # new_string = new_string.str.replace('-','')\r\n    df['clean_course_title'] = new_string\r\n    # regex = re.compile('[^a-zA-Z]')\r\n    # df['clean_course_title'] = regex.sub('',df['clean_course_title'])\r\n\r\n    # print(df[['clean_course_title','course_title']])\r\n    #\r\n    print(\"Improt Tfidf\")\r\n    Xfeatures = df['clean_course_title']\r\n    ylabels = df['subject']\r\n    tfidf_vec = TfidfVectorizer()\r\n    X = tfidf_vec.fit_transform(Xfeatures.values.astype('U'))\r\n\r\n    print(\"Split Dataset\")\r\n    # # Split our dataset\r\n    x_train, x_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.2, random_state=42)\r\n    #\r\n    #\r\n    # # Build Model\r\n    lr_model = LogisticRegression(warm_start=True)\r\n    print(\"Fit Model\")\r\n    lr_model.fit(x_train, y_train)\r\n    #\r\n    print(lr_model.score(x_test, y_test))\r\n\r\n    print(\"Open Pickle\")\r\n    pickle.dump(tfidf_vec, open('tfidf_vec.pickle', 'wb'))\r\n\r\n    # # save the model to disk\r\n    print(\"Save model to disk\")\r\n    filename = 'finalized_model.sav'\r\n    pickle.dump(lr_model, open(filename, 'wb'))\r\n\r\n    print(\"Save Model\")\r\n    model_create = models.Provider_Model(model_name=filename, accuracy_score=str(lr_model.score(x_test, y_test)),\r\n                                         model_location='drive C')\r\n    model_create.save()\r\n\r\n    # load the model from disk\r\n    print(\"Load Model\")\r\n    loaded_model = pickle.load(open(filename, 'rb'))\r\n    result = loaded_model.score(x_test, y_test)\r\n    print(result)\r\n    from sklearn.metrics import classification_report, confusion_matrix, plot_confusion_matrix\r\n\r\n    y_pred = lr_model.predict(x_test)\r\n\r\n    # Confusion Matrix : true pos,false pos,etc\r\n    # print(confusion_matrix(y_pred,y_test))\r\n    # print(df['subject'].unique())\r\n    # print(classification_report(y_pred,y_test))\r\n    # plot_confusion_matrix(lr_model,x_test,y_test)\r\n\r\n    ### Making A Single Prediction\r\n    ex = \"RS. CIPUTRA CITRA GARDEN CITY\"\r\n\r\n    def vectorize_text(text):\r\n        my_vec = tfidf_vec.transform([text])\r\n        return my_vec.toarray()\r\n\r\n    vectorize_text(ex)\r\n    df.to_excel('wew.xlsx')\r\n    sample1 = vectorize_text(ex)\r\n    pred = lr_model.predict(sample1)\r\n\r\n    print(pred)\r\n    print(\"Finish Creating Model\")\r\n\r\ndef create_model_bc(df):\r\n    print(\"Create Model\")\r\n    lr_model = LogisticRegression(solver='liblinear')\r\n    df['clean_course_title'] = df['course_title'].astype(str)\r\n    # df['clean_course_title'] = df['clean_course_title'].apply(lambda x: ' '.join([ps.stem(word) for word in x.split() if word not in set(all_stopwords)]))\r\n    df['clean_course_title'] = df['clean_course_title'].fillna('').astype(str).replace('', np.nan, regex=False)\r\n    new_string = df['clean_course_title'].str.replace('.', '')\r\n    new_string = new_string.str.lower()\r\n    new_string = new_string.str.replace('&', '')\r\n    # new_string = new_string.str.replace('-','')\r\n    df['clean_course_title'] = new_string\r\n    # regex = re.compile('[^a-zA-Z]')\r\n    # df['clean_course_title'] = regex.sub('',df['clean_course_title'])\r\n\r\n    # print(df[['clean_course_title','course_title']])\r\n    #\r\n    print(\"Improt Tfidf\")\r\n    Xfeatures = df['clean_course_title']\r\n    ylabels = df['subject']\r\n    tfidf_vec = TfidfVectorizer()\r\n    X = tfidf_vec.fit_transform(Xfeatures.values.astype('U'))\r\n\r\n    print(\"Split Dataset\")\r\n    # # Split our dataset\r\n    x_train, x_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.3, random_state=42)\r\n    #\r\n    #\r\n    # # Build Model\r\n    print(\"Fit Model\")\r\n    lr_model.fit(x_train, y_train)\r\n    #\r\n    print(lr_model.score(x_test, y_test))\r\n\r\n    print(\"Open Pickle\")\r\n    pickle.dump(tfidf_vec, open('tfidf_vec.pickle', 'wb'))\r\n\r\n    # # save the model to disk\r\n    print(\"Save model to disk\")\r\n    filename = 'finalized_model.sav'\r\n    pickle.dump(lr_model, open(filename, 'wb'))\r\n\r\n    print(\"Save Model\")\r\n    model_create = models.Provider_Model(model_name=filename, accuracy_score=str(lr_model.score(x_test, y_test)),\r\n                                         model_location='drive C')\r\n    model_create.save()\r\n\r\n    # load the model from disk\r\n    print(\"Load Model\")\r\n    loaded_model = pickle.load(open(filename, 'rb'))\r\n    result = loaded_model.score(x_test, y_test)\r\n    print(result)\r\n    from sklearn.metrics import classification_report, confusion_matrix, plot_confusion_matrix\r\n\r\n    y_pred = lr_model.predict(x_test)\r\n\r\n    # Confusion Matrix : true pos,false pos,etc\r\n    # print(confusion_matrix(y_pred,y_test))\r\n    # print(df['subject'].unique())\r\n    # print(classification_report(y_pred,y_test))\r\n    # plot_confusion_matrix(lr_model,x_test,y_test)\r\n\r\n    ### Making A Single Prediction\r\n    ex = \"RS. CIPUTRA CITRA GARDEN CITY\"\r\n\r\n    def vectorize_text(text):\r\n        my_vec = tfidf_vec.transform([text])\r\n        return my_vec.toarray()\r\n\r\n    vectorize_text(ex)\r\n    df.to_excel('wew.xlsx')\r\n    sample1 = vectorize_text(ex)\r\n    pred = lr_model.predict(sample1)\r\n\r\n    print(pred)\r\n    print(\"Finish Creating Model\")\r\n\r\n\r\ndef create_model(dfc):\r\n    print(\"Create Model\")\r\n    # lr_model = LogisticRegression(solver='sag',warm_start=True)\r\n    lr_model = SGDClassifier(loss='modified_huber',learning_rate='constant',n_jobs=-1,random_state=0,eta0=0.1)\r\n    pembersih = Pembersih(dfc)\r\n    df = pembersih._return_df()\r\n    df['clean_course_title'] = df['course_title']\r\n    # df['clean_course_title'] = df['clean_course_title'].fillna('').astype(str).replace('', np.nan, regex=False)\r\n    new_string = df['clean_course_title']\r\n    # new_string = new_string.str.lower()\r\n    # new_string = new_string.str.replace('&', '')\r\n    # new_string = new_string.str.replace('-','')\r\n    # df['clean_course_title'] = new_string\r\n    # print(df)\r\n\r\n\r\n    print(\"Improt Tfidf\")\r\n    Xfeatures = df['clean_course_title']\r\n    ylabels = df['subject']\r\n    tfidf_vec = TfidfVectorizer()\r\n    X = tfidf_vec.fit_transform(Xfeatures.values.astype('U'))\r\n    print(\"Split Dataset \")\r\n\r\n    x_train, x_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.2, random_state=1)\r\n    # print(X.shape)\r\n    try:\r\n        print(\"Fit Model\")\r\n        lr_model.partial_fit(x_train, y_train, classes=np.unique(ylabels))\r\n        # calibrator = CalibratedClassifierCV(clf, cv='prefit')\r\n        # model = calibrator.fit(X_tr, y_train)\r\n        print(lr_model.score(x_test, y_test))\r\n\r\n    except Exception as e:\r\n        print(\"sumting wonge \"+str(e))\r\n\r\n\r\n\r\n\r\n\r\n    print(\"Open Pickle\")\r\n    pickle.dump(tfidf_vec, open('tfidf_vec.pickle', 'wb'))\r\n\r\n    # # save the model to disk\r\n    print(\"Save model to disk\")\r\n    filename = 'finalized_model.sav'\r\n    pickle.dump(lr_model, open(filename, 'wb'))\r\n\r\n    print(\"Save Model\")\r\n    model_create = models.Provider_Model(model_name=filename, accuracy_score=str(lr_model.score(x_test, y_test)),\r\n                                         model_location='drive C')\r\n    model_create.save()\r\n\r\n    # load the model from disk\r\n    print(\"Load Model\")\r\n    loaded_model = pickle.load(open(filename, 'rb'))\r\n    result = loaded_model.score(x_test, y_test)\r\n    print(result)\r\n    from sklearn.metrics import classification_report, confusion_matrix, plot_confusion_matrix\r\n\r\n\r\n\r\n    ## Making A Single Prediction\r\n    ex = \"RS. CIPUTRA CITRA GARDEN CITY\"\r\n\r\n    def vectorize_text(text):\r\n        my_vec = tfidf_vec.transform([text])\r\n        return my_vec.toarray()\r\n\r\n    vectorize_text(ex)\r\n    df.to_excel('wew.xlsx')\r\n    sample1 = vectorize_text(ex)\r\n    pred = lr_model.predict(sample1)\r\n    #\r\n    print(pred)\r\n    print(\"Finish Creating Model\")\r\n\r\n\r\n\r\ndef upload_file_train(request):\r\n    df = pd.read_excel(\"dataset_excel_copy.xlsx\")\r\n    create_model(df)\r\n\r\n\r\n    list_model = models.Provider_Model.objects.all()\r\n    context_all = {'list_model': list_model}\r\n\r\n    return render(request, 'model/model.html', context=context_all)\r\n\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/model/views.py b/model/views.py
--- a/model/views.py	(revision fc9698fd5cf49d7a238f0a8c1d2cd7aa8832cb2d)
+++ b/model/views.py	(date 1677049951343)
@@ -520,18 +520,11 @@
 
 def create_model(dfc):
     print("Create Model")
-    # lr_model = LogisticRegression(solver='sag',warm_start=True)
     lr_model = SGDClassifier(loss='modified_huber',learning_rate='constant',n_jobs=-1,random_state=0,eta0=0.1)
     pembersih = Pembersih(dfc)
     df = pembersih._return_df()
     df['clean_course_title'] = df['course_title']
-    # df['clean_course_title'] = df['clean_course_title'].fillna('').astype(str).replace('', np.nan, regex=False)
-    new_string = df['clean_course_title']
-    # new_string = new_string.str.lower()
-    # new_string = new_string.str.replace('&', '')
-    # new_string = new_string.str.replace('-','')
-    # df['clean_course_title'] = new_string
-    # print(df)
+
 
 
     print("Improt Tfidf")
Index: .idea/workspace.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+><?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n<project version=\"4\">\r\n  <component name=\"AutoImportSettings\">\r\n    <option name=\"autoReloadType\" value=\"SELECTIVE\" />\r\n  </component>\r\n  <component name=\"ChangeListManager\">\r\n    <list default=\"true\" id=\"7ec4b836-b3a2-487c-8ac7-00cf0b5546fa\" name=\"Changes\" comment=\"er\" />\r\n    <option name=\"SHOW_DIALOG\" value=\"false\" />\r\n    <option name=\"HIGHLIGHT_CONFLICTS\" value=\"true\" />\r\n    <option name=\"HIGHLIGHT_NON_ACTIVE_CHANGELIST\" value=\"false\" />\r\n    <option name=\"LAST_RESOLUTION\" value=\"IGNORE\" />\r\n  </component>\r\n  <component name=\"FileTemplateManagerImpl\">\r\n    <option name=\"RECENT_TEMPLATES\">\r\n      <list>\r\n        <option value=\"Python Script\" />\r\n        <option value=\"HTML File\" />\r\n      </list>\r\n    </option>\r\n  </component>\r\n  <component name=\"Git.Settings\">\r\n    <option name=\"RECENT_BRANCH_BY_REPOSITORY\">\r\n      <map>\r\n        <entry key=\"$PROJECT_DIR$\" value=\"multi_proc\" />\r\n      </map>\r\n    </option>\r\n    <option name=\"RECENT_GIT_ROOT_PATH\" value=\"$PROJECT_DIR$\" />\r\n    <option name=\"RESET_MODE\" value=\"HARD\" />\r\n  </component>\r\n  <component name=\"MarkdownSettingsMigration\">\r\n    <option name=\"stateVersion\" value=\"1\" />\r\n  </component>\r\n  <component name=\"ProjectId\" id=\"2FwDkPSADAPfsz8IXwx1GemBeRE\" />\r\n  <component name=\"ProjectLevelVcsManager\" settingsEditedManually=\"true\" />\r\n  <component name=\"ProjectViewState\">\r\n    <option name=\"hideEmptyMiddlePackages\" value=\"true\" />\r\n    <option name=\"showLibraryContents\" value=\"true\" />\r\n  </component>\r\n  <component name=\"PropertiesComponent\">{\r\n  &quot;keyToString&quot;: {\r\n    &quot;DefaultHtmlFileTemplate&quot;: &quot;HTML File&quot;,\r\n    &quot;RunOnceActivity.OpenProjectViewOnStart&quot;: &quot;true&quot;,\r\n    &quot;RunOnceActivity.ShowReadmeOnStart&quot;: &quot;true&quot;,\r\n    &quot;last_opened_file_path&quot;: &quot;C:/Users/User/PycharmProjects/nlp_provider/templates/matching&quot;,\r\n    &quot;settings.editor.selected.configurable&quot;: &quot;com.jetbrains.python.configuration.PyActiveSdkModuleConfigurable&quot;\r\n  },\r\n  &quot;keyToStringList&quot;: {\r\n    &quot;com.intellij.ide.scratch.ScratchImplUtil$2/New Scratch File&quot;: [\r\n      &quot;HTML&quot;\r\n    ]\r\n  }\r\n}</component>\r\n  <component name=\"RecentsManager\">\r\n    <key name=\"CopyFile.RECENT_KEYS\">\r\n      <recent name=\"C:\\Users\\User\\PycharmProjects\\nlp_provider\\templates\\matching\" />\r\n      <recent name=\"C:\\Users\\User\\PycharmProjects\\nlp_provider\\templates\\master\" />\r\n      <recent name=\"C:\\Users\\User\\PycharmProjects\\nlp_provider\\templates\" />\r\n    </key>\r\n  </component>\r\n  <component name=\"SpellCheckerSettings\" RuntimeDictionaries=\"0\" Folders=\"0\" CustomDictionaries=\"0\" DefaultDictionary=\"application-level\" UseSingleDictionary=\"true\" transferred=\"true\" />\r\n  <component name=\"TaskManager\">\r\n    <task active=\"true\" id=\"Default\" summary=\"Default task\">\r\n      <changelist id=\"8dd27ab8-2156-4473-a4f2-ffa627ae7979\" name=\"Changes\" comment=\"er\" />\r\n      <created>1665394394507</created>\r\n      <option name=\"number\" value=\"Default\" />\r\n      <option name=\"presentableId\" value=\"Default\" />\r\n      <updated>1665394394507</updated>\r\n    </task>\r\n    <servers />\r\n  </component>\r\n  <component name=\"Vcs.Log.Tabs.Properties\">\r\n    <option name=\"TAB_STATES\">\r\n      <map>\r\n        <entry key=\"MAIN\">\r\n          <value>\r\n            <State />\r\n          </value>\r\n        </entry>\r\n      </map>\r\n    </option>\r\n  </component>\r\n  <component name=\"VcsManagerConfiguration\">\r\n    <MESSAGE value=\"er\" />\r\n    <option name=\"LAST_COMMIT_MESSAGE\" value=\"er\" />\r\n  </component>\r\n</project>
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/workspace.xml b/.idea/workspace.xml
--- a/.idea/workspace.xml	(revision fc9698fd5cf49d7a238f0a8c1d2cd7aa8832cb2d)
+++ b/.idea/workspace.xml	(date 1677053273105)
@@ -4,7 +4,12 @@
     <option name="autoReloadType" value="SELECTIVE" />
   </component>
   <component name="ChangeListManager">
-    <list default="true" id="7ec4b836-b3a2-487c-8ac7-00cf0b5546fa" name="Changes" comment="er" />
+    <list default="true" id="7ec4b836-b3a2-487c-8ac7-00cf0b5546fa" name="Changes" comment="er">
+      <change beforePath="$PROJECT_DIR$/.idea/workspace.xml" beforeDir="false" afterPath="$PROJECT_DIR$/.idea/workspace.xml" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/model/views.py" beforeDir="false" afterPath="$PROJECT_DIR$/model/views.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/nlp_provider/utils.py" beforeDir="false" afterPath="$PROJECT_DIR$/nlp_provider/utils.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/nlp_provider/views.py" beforeDir="false" afterPath="$PROJECT_DIR$/nlp_provider/views.py" afterDir="false" />
+    </list>
     <option name="SHOW_DIALOG" value="false" />
     <option name="HIGHLIGHT_CONFLICTS" value="true" />
     <option name="HIGHLIGHT_NON_ACTIVE_CHANGELIST" value="false" />
